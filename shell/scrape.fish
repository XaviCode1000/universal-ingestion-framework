# scrape.fish - Wrapper CLI para Universal Ingestion Framework
# Motor de ingesta de conocimiento de alta fidelidad para LLMs y sistemas RAG

function scrape -d "UIF - TransformÃ¡ contenido web a Markdown"
    # Especificaciones de opciones
    argparse -n scrape \
        'h/help' \
        's/setup' \
        'u/url=' \
        'c/config=' \
        'w/workers=' \
        'd/output-dir=' \
        'scope=' \
        'o/only-text' \
        'v/verbose' \
        'q/quiet' \
        'dry-run' \
        'list-scopes' \
        -- $argv

    # Manejar errores de parseo
    or return 1

    # Directorio del proyecto
    set -l project_dir "$HOME/Dev/my_apps/universal-ingestion-framework"

    # Mostrar ayuda
    if set -ql _flag_help[1]
        _scrape_show_help
        return 0
    end

    # Listar scopes disponibles
    if set -ql _flag_list_scopes[1]
        echo "Scopes disponibles:"
        echo "  smart  - Auto-detectar: dominio raÃ­z = broad, subdirectorio = strict"
        echo "  strict - Solo URLs bajo la ruta de la URL semilla"
        echo "  broad  - Todas las URLs dentro del mismo dominio"
        return 0
    end

    # Construir comando
    set -l cmd uv run uif-scraper

    # Modo setup
    if set -ql _flag_setup[1]
        echo "ðŸ›¸ Iniciando asistente de configuraciÃ³n de UIF Scraper..."
        cd $project_dir
        eval $cmd --setup
        return $status
    end

    # Archivo de configuraciÃ³n
    if set -ql _flag_config[1]
        set -a cmd --config $_flag_config
    end

    # Workers
    if set -ql _flag_workers[1]
        set -a cmd --workers $_flag_workers
    end

    # Directorio de salida
    if set -ql _flag_output_dir[1]
        set -a cmd --output-dir $_flag_output_dir
    end

    # Scope
    if set -ql _flag_scope[1]
        set -a cmd --scope $_flag_scope
    end

    # Solo texto (sin assets)
    if set -ql _flag_only_text[1]
        set -a cmd --only-text
    end

    # URL desde flag o argumento posicional
    set -l target_url
    if set -ql _flag_url[1]
        set target_url $_flag_url
    else if test (count $argv) -gt 0
        set target_url $argv[1]
    end

    # Validar que se proporcionÃ³ URL (salvo en dry-run o setup)
    if not set -ql _flag_dry_run[1]
        if test -z "$target_url"
            echo "Error: La URL es requerida." >&2
            echo "Uso: scrape <URL> [opciones]" >&2
            echo "EjecutÃ¡ 'scrape --help' para mÃ¡s informaciÃ³n." >&2
            return 1
        end
    end

    # Directorio de salida por defecto
    set -l output_dir (set -ql _flag_output_dir[1] && echo $_flag_output_dir || echo "./data")

    # Dry run - mostrar quÃ© se ejecutarÃ­a
    if set -ql _flag_dry_run[1]
        echo "ðŸ“‹ Dry run - comando que se ejecutarÃ­a:"
        echo "  cd $project_dir"
        echo "  $cmd $target_url"
        echo ""
        echo "ðŸ“ Directorio de salida: $output_dir"
        return 0
    end

    # Modo verbose
    if set -ql _flag_verbose[1]
        echo "ðŸš€ Iniciando UIF Scraper..."
        echo "   URL: $target_url"
        echo "   Directorio de salida: $output_dir"
        echo "   Workers: "(set -ql _flag_workers[1] && echo $_flag_workers || echo "por defecto (5)")
        echo "   Scope: "(set -ql _flag_scope[1] && echo $_flag_scope || echo "smart")
        echo "   Assets: "(set -ql _flag_only_text[1] && echo "desactivados" || echo "activados")
        echo ""
    end

    # Modo quiet
    if set -ql _flag_quiet[1]
        set -a cmd 2>/dev/null
    end

    # Ejecutar
    cd $project_dir
    eval $cmd $target_url

    return $status
end

function _scrape_show_help
    echo ""
    echo "ðŸ›¸ Universal Ingestion Framework (UIF) v3.0"
    echo ""
    echo "USO"
    echo "    scrape <URL> [OPCIONES]"
    echo "    scrape --setup"
    echo ""
    echo "DESCRIPCIÃ“N"
    echo "    Motor de ingesta de conocimiento de alta fidelidad que transforma"
    echo "    contenido web y documentos binarios a Markdown optimizado para"
    echo "    LLMs y sistemas RAG."
    echo ""
    echo "OPCIONES"
    echo "    -h, --help              Mostrar este mensaje de ayuda"
    echo "    -s, --setup             Iniciar asistente de configuraciÃ³n interactivo"
    echo "    -u, --url=URL           URL objetivo a scrapear"
    echo "    -d, --output-dir=DIR    Directorio de salida (por defecto: ./data)"
    echo "    -c, --config=ARCH       Usar archivo de configuraciÃ³n custom (YAML)"
    echo "    -w, --workers=N         NÃºmero de workers concurrentes (por defecto: 5)"
    echo "    --scope=SCOPE           Alcance del rastreo: smart, strict, broad (defecto: smart)"
    echo "    -o, --only-text         Saltar descarga de assets (imÃ¡genes, PDFs)"
    echo "    -v, --verbose           Mostrar informaciÃ³n detallada de ejecuciÃ³n"
    echo "    -q, --quiet             Suprimir salida que no sean errores"
    echo "    --dry-run               Mostrar comando sin ejecutarlo"
    echo "    --list-scopes           Mostrar opciones de scope disponibles"
    echo ""
    echo "OPCIONES DE SCOPE"
    echo "    smart   Auto-detectar basado en la URL semilla:"
    echo "            - Dominio raÃ­z (ej: example.com/) â†’ scope broad"
    echo "            - Subdirectorio (ej: example.com/blog/) â†’ scope strict"
    echo ""
    echo "    strict  Solo rastrear URLs que comienzan con la ruta de la URL semilla."
    echo "            Ideal para scrapear secciones especÃ­ficas de un sitio."
    echo ""
    echo "    broad   Rastrear todas las URLs dentro del mismo dominio."
    echo "            Usar con precauciÃ³n en sitios grandes."
    echo ""
    echo "EJEMPLOS"
    echo "    # Uso bÃ¡sico (guarda en ./data)"
    echo "    scrape https://example.com"
    echo ""
    echo "    # Especificar directorio de salida personalizado"
    echo "    scrape https://example.com -d ~/Documentos/scraping"
    echo ""
    echo "    # Usar archivo de configuraciÃ³n personalizado"
    echo "    scrape https://example.com -c ~/.config/uif-scraper/config.yaml"
    echo ""
    echo "    # Con workers y scope personalizados"
    echo "    scrape https://example.com/blog --workers 10 --scope strict"
    echo ""
    echo "    # Solo texto, sin assets"
    echo "    scrape https://docs.python.org/3/ --only-text"
    echo ""
    echo "    # ConfiguraciÃ³n interactiva (genera config.yaml)"
    echo "    scrape --setup"
    echo ""
    echo "    # Dry run para previsualizar el comando"
    echo "    scrape https://example.com --dry-run"
    echo ""
    echo "ARCHIVO DE CONFIGURACIÃ“N"
    echo "    Ubicaciones buscadas (en orden):"
    echo "    1. Ruta especificada con --config"
    echo "    2. \$XDG_CONFIG_HOME/uif-scraper/config.yaml"
    echo "    3. ~/.config/uif-scraper/config.yaml"
    echo "    4. /etc/uif-scraper/config.yaml"
    echo ""
    echo "    Opciones disponibles en config.yaml:"
    echo "    data_dir: ~/scraping-data          # Directorio de salida"
    echo "    default_workers: 5                 # Workers para pÃ¡ginas"
    echo "    asset_workers: 8                   # Workers para assets"
    echo "    max_retries: 3                     # Reintentos mÃ¡ximos"
    echo "    timeout_seconds: 30                # Timeout por request"
    echo "    log_level: INFO                    # DEBUG, INFO, WARNING, ERROR"
    echo "    db_pool_size: 5                    # Pool de conexiones SQLite"
    echo ""
    echo "    Ver config.example.yaml en el proyecto para ejemplo completo."
    echo ""
    echo "ESTRUCTURA DE SALIDA"
    echo "    <output-dir>/"
    echo "    â””â”€â”€ example_com/"
    echo "        â”œâ”€â”€ content/              # Archivos Markdown (.md.zst comprimidos)"
    echo "        â”œâ”€â”€ media/"
    echo "        â”‚   â”œâ”€â”€ images/           # ImÃ¡genes descargadas"
    echo "        â”‚   â””â”€â”€ docs/             # PDFs + .md convertidos"
    echo "        â””â”€â”€ state.db              # Estado SQLite (modo WAL, batched)"
    echo ""
    echo "FEATURES"
    echo "    - ExtracciÃ³n de texto con 4 niveles de fallback (Trafilatura â†’ MarkItDown â†’ BS4)"
    echo "    - CompresiÃ³n Zstandard para markdown (30-40% mÃ¡s chico)"
    echo "    - Procesamiento batch async con connection pooling"
    echo "    - SQLite en modo WAL con reintento automÃ¡tico en locks"
    echo ""
    echo "DOCUMENTACIÃ“N"
    echo "    Proyecto: ~/Dev/my_apps/universal-ingestion-framework"
    echo "    Changelog: docs/CHANGELOG.md"
    echo ""
    echo "AUTOR"
    echo "    UIF Scraper - Universal Ingestion Framework"
    echo ""
end
